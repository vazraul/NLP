{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vazraul/NLP/blob/main/M06NB01_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jpZ1j2YBmLh"
      },
      "source": [
        "# <center>VAI Academy - Módulo 6</center>\n",
        "# <center>NLP</center>\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxVoqSQV3xRF"
      },
      "source": [
        "## Conteúdo\n",
        "1. [Introdução](#intro) <br>\n",
        "2. [Conceitos Gerais](#conc_gerais) <br>\n",
        "3. [Análise de Sentimentos](#analise_sent) <br>\n",
        "4. [Estado da Arte em NLP](#sota_nlp) <br>\n",
        "5. [Dicas e Referências](#digdeeper)\n",
        "\n",
        "<a id=\"dsprecap\"></a>\n",
        "## Data Science Pipeline (DSP) recap\n",
        "1. Definição do Problema / Definição do Escopo\n",
        "2. Definição das Métricas de Sucesso\n",
        "3. Definição dos Dados Necessários\n",
        "4. Aquisição de Dados\n",
        "5. Pré-processamento de Dados\n",
        "6. Análise Exploratória de Dados (E.D.A.)\n",
        "7. <i>Feature Engineering</i>\n",
        "8. Construção e Avaliação do Modelo\n",
        "9. Comunicação dos Resultados\n",
        "10. Implantação\n",
        "11. Monitoramento e Manutenção"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqI0vjZHu9gO"
      },
      "source": [
        "<a id=\"intro\"></a>\n",
        "# 1 Introdução\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJM3frleTZBj"
      },
      "source": [
        "## 1.1 O que é NLP e onde posso usá-lo?\n",
        "\n",
        "NLP (Natural Language Processing, ou, Processamento de Linguagem Natural) é um campo de estudo que reúne conceitos de linguística, ciência da computação e inteligência artificial para criar e aplicar ferramentas a problemas do mundo real relativos a dados de linguagem natural humana. Usamos soluções que implementam tais técnicas todos os dias com ferramentas como o motor de busca e tradutor do Google, sugestões de texto do Grammarly, autocorreções de texto do Microsoft Word, páginas de FAQs, entre outras. Por trás de todas essas ferramentas está o objetivo principal do NLP: fazer com que os computadores entendam a linguagem natural humana e trabalhem com ela para construir soluções.\n",
        "\n",
        "É bom observar que o conceito de linguagem natural é aplicado a todas as maneiras pelas quais os humanos podem se comunicar. Os conceitos do NLP foram desenvolvidos ao longo dos anos para trabalhar com dados como texto ou fala. Hoje em dia é aplicado a muitos negócios e estão gerando muito valor. As empresas fazem cada vez mais produtos baseados em NLP e é uma habilidade muito exigida para Cientistas de Dados e Engenheiros de Machine Learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R95FE1GWX0AE"
      },
      "source": [
        "## 1.2 Como posso começar a construir aplicativos de NLP?\n",
        "\n",
        "Como começar a construir um aplicativo de NLP depende do que você deseja alcançar. Para alguém que é membro de uma equipe sem nenhum conhecimento de programação, mas deseja começar a usar soluções de NLP, recomendamos fortemente que dê uma olhada nas excelentes ferramentas que a Amazon AWS, Microsoft Azure e Google Cloud tem a oferecer. Elas geralmente são baratas e fáceis de usar. Mesmo que não sirva para gerar seu produto, certamente pode servir como uma prova de conceito antes de você contratar uma equipe de desenvolvedores para construir sua solução.\n",
        "\n",
        "Se é para alguém que sabe programar, mas deseja apenas a solução e não o processo de aprendizagem, pode-se usar os repositórios do GitHub para obter soluções de alta qualidade que possam ser integradas por você ao seu produto. Posteriormente nesta aula, mostraremos alguns exemplos de grupos que desenvolvem aplicativos de NLP de última geração e de código aberto.\n",
        "\n",
        "Mas se falamos de alguém que deseja desenvolver suas soluções e desenvolver sua equipe para se tornar bom nisso, pode-se usar o Python. Essa linguagem de programação é uma velha amiga dos Cientistas de Dados e Engenheiros de Machine Learning. Muitas ferramentas para lidar com tarefas de NLP foram adicionadas ao Python em uma forma muito semelhante às ferramentas e bibliotecas padrão de Data Science e Machine Learning. Então, dado que você está familiarizado com o SKLearn e o Pandas, você está na metade do caminho para começar a construir soluções com NLP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3iXobmpXyKX"
      },
      "source": [
        "Bem, vamos começar a aprender!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGScEtymuwvH"
      },
      "source": [
        "<a id=\"conc_gerais\"></a>\n",
        "# 2 Conceitos Gerais\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhN9w9OJYaJr"
      },
      "source": [
        "## 2.1 Correspondência de texto (Text Matching)\n",
        "\n",
        "Para começar, vamos falar sobre correspondência de texto. Se você nunca trabalhou com NLP, pode estar adivinhando \"Bem, é apenas correspondência de strings, certo?\". A resposta é que, na verdade, essa é só uma das maneiras de executar correspondência de texto, mas existem muitas outras técnicas que podemos explorar para isso. Aqui, abordaremos duas maneiras poderosas de realizar correspondência de texto: **Regex** e **Fuzzy Text Matching**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSp-JRYpZGwQ"
      },
      "source": [
        "### 2.1.1 Regex\n",
        "\n",
        "Regex significa Expressão Regular (Regular Expression). Ele consiste em um padrão que você pode definir, explicando ao seu computador que tipo de texto você deseja pesquisar. Imagine o regex como uma correspondência de strings aprimorada. Aqui você estará comparando strings, mas de uma forma inteligente. Vamos dar um exemplo.\n",
        "\n",
        "Suponha que você deseje detectar todos os números em um texto. Com a correspondência de texto por regex, você pode pesquisar cada dígito individual e fazer toda uma lógica para obter seus resultados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaR_hn1lg05h",
        "cellView": "both"
      },
      "source": [
        "# texto de input utilizado\n",
        "input_text = \"Temos 2 pizzas em uma de nossas 5 geladeiras.\"\n",
        "\n",
        "# definindo uma lista de dígitos\n",
        "digit_list = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "\n",
        "# vamos fazer uma lista com os dígitos encontrados no nosso input_text\n",
        "digits_found = []\n",
        "\n",
        "# para cada char no nosso input_text, vamos verificar se ele está presente na nossa lista de dígitos\n",
        "for character in input_text:\n",
        "  # pegamos somente os chars que estão na lista de dígitos\n",
        "  if character in digit_list:\n",
        "    digits_found.append(character)\n",
        "\n",
        "digits_found\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIIN5_RiaZ5r"
      },
      "source": [
        "Parece que funcionou bem para esse caso, mas vamos testar mais um pouco."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-og9DtlpaXPY",
        "cellView": "both"
      },
      "source": [
        "# texto de input utilizado\n",
        "input_text = \"Temos 2 pizzas em uma de nossas 556 geladeiras.\"\n",
        "\n",
        "# definindo uma lista de dígitos\n",
        "digit_list = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "\n",
        "# vamos fazer uma lista com os dígitos encontrados no nosso input_text\n",
        "digits_found = []\n",
        "\n",
        "# para cada char no nosso input_text, vamos verificar se ele está presente na nossa lista de dígitos\n",
        "for character in input_text:\n",
        "  # pegamos somente os chars que estão na lista de dígitos\n",
        "  if character in digit_list:\n",
        "    digits_found.append(character)\n",
        "\n",
        "digits_found\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWsa-Mawa6a6"
      },
      "source": [
        "Nossa lógica falhou. O número \"556\" foi retornado como três valores separados e não é o que queríamos. Como você pode imaginar, será necessária outra lógica inteira para executar nossa tarefa.\n",
        "\n",
        "Antes de começar a programar, vamos dar uma chance ao regex.\n",
        "\n",
        "Como explicamos antes, o regex permite que você obtenha correspondências em um texto de uma maneira muito fácil. Vamos tentar um exemplo muito simples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljey5jmga5AL",
        "cellView": "both"
      },
      "source": [
        "# o pacote re do Python é usado para regex\n",
        "import re\n",
        "\n",
        "input_text = \"Deixei meu celular no meu carro\"\n",
        "\n",
        "# vamos procurar por \"celular\" no text\n",
        "# regex retorna a seguinte correspondência\n",
        "re.findall(\"celular\", input_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZX6nB8xcqNV"
      },
      "source": [
        "A beleza do regex é que ele nos permite usar **Sequências Especiais** para corresponder o texto de uma maneira mais inteligente. Uma sequência especial muito útil é **\\d**. Você pode usá-la para combinar dígitos de uma forma muito simples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrDl81r4cZI9",
        "cellView": "both"
      },
      "source": [
        "import re\n",
        "\n",
        "# vamor pegar o nosso texto de input\n",
        "input_text = \"There are 2 pizza in one of our 556 fridges.\"\n",
        "\n",
        "# vamos procurar por dígitos no texto\n",
        "re.findall(\"\\d\", input_text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_-M3orwd2SY"
      },
      "source": [
        "Bem, temos a mesma resposta do programa de antes, com muito menos código, mas ainda estamos obtendo um resultado errado. Isso porque, por padrão, o regex está pesquisando dígitos individualmente. Não adicionamos a inteligência para combinar números com qualquer comprimento. Isso pode ser feito facilmente usando **Metacaracteres**.\n",
        "\n",
        "Metacaracteres especificam certas regras sobre nossa expressão. Para esse caso, podemos dizer à nossa expressão para combinar uma ou mais ocorrências de dígitos com o metacaractere **+**. Vamos tentar. Agora nossa expressão entenderá números de comprimento variável."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUBxAM8UdKx2"
      },
      "source": [
        "import re\n",
        "\n",
        "# vamos pegar o nosso input\n",
        "input_text = \"There are 2 pizza in one of our 556 fridges.\"\n",
        "\n",
        "# vamos procurar por dígitos no texto\n",
        "re.findall(\"\\d+\", input_text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PX5lKQM1eqSm"
      },
      "source": [
        "De uma forma fácil, conseguimos pegar cada número da nossa string.\n",
        "\n",
        "Vamos tentar outro desafio: Encontrar datas no nosso texto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxNQLQ3xehFv"
      },
      "source": [
        "import re\n",
        "\n",
        "# vamos pegar o nosso texto de input\n",
        "input_text = \"Minha data de nascimento é 26/07/1978\"\n",
        "\n",
        "# vamos procurar por datas no texto\n",
        "re.findall(\"\\d{2}/\\d{2}/\\d{4}\", input_text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-LKGH3GfH0C"
      },
      "source": [
        "Nossa expressão está dizendo \"encontre 2 dígitos seguidos de uma barra, seguida de dois dígitos, seguida de outra barra, seguida de quatro dígitos\". Podemos usar o metacaractere {} para especificar o número exato de ocorrências para os dígitos.\n",
        "\n",
        "Vamos imaginar que queremos uma maneira fácil de obter o dia, o mês e o ano individualmente. Com regex, uma maneira fácil de fazer isso é usando **Grupos**. Podemos definir o entorno de uma seção de nossa expressão com ()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qO61_I-IfGxD"
      },
      "source": [
        "import re\n",
        "\n",
        "# vamos pegar o nosso texto de input\n",
        "input_text = \"Minha data de aniversário é 26/07/1978 e do meu irmão é 21/05/1998\"\n",
        "\n",
        "# vamos procurar por datas no texto\n",
        "re.findall(\"(\\d{2})/(\\d{2})/(\\d{4})\", input_text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXu4pjGEg93n"
      },
      "source": [
        "Observe como, para cada correspondência, obtivemos uma tupla com nossas informações de data divididas individualmente.\n",
        "\n",
        "Ótimo, fizemos uma boa revisão geral do regex. Lembre-se de que existem muitos metacaracteres e sequências especiais para você explorar. Por enquanto, vamos prosseguir para a correspondência de texto difusa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoSfsrHPiMrk"
      },
      "source": [
        "### 2.1.2 Fuzzy Text Matching\n",
        "\n",
        "Em aplicações de Data Science do mundo real, é muito comum trabalhar com probabilidades. Por enquanto, trabalhamos apenas com correspondência de texto binária. Ou duas strings são idênticas ou não. Agora, aprenderemos técnicas mais sofisticadas para comparar textos. Não obteremos respostas como \"sim\" ou \"não\" quando tivermos duas strings, mas obteremos respostas como \"Esta string é 87% semelhante à outra\". Pode parecer que estamos complicando as coisas, mas imagine que estamos construindo um FAQ inteligente. Um usuário perguntou \"Meu computador quebrou\" e outro perguntou \"Acho que meu computador quebrou\". Nosso FAQ deve dar a esses usuários a mesma resposta e, para isso, precisamos que nosso aplicativo entenda que essas duas frases são muito semelhantes.\n",
        "\n",
        "Em Python, existem duas bibliotecas que podem ajudá-lo muito a fazer correspondência de texto difuso: **jellyfish** e **fuzzywuzzy**. Dentro delas, existem vários métodos estatísticos para comparar duas frases. Não entraremos na matemática dos algoritmos aqui, mas vale testar as seguintes métricas para tentar ver qual funciona melhor com seu aplicativo.\n",
        "\n",
        "**Jellyfish**\n",
        "\n",
        "\n",
        "*   Levenshtein Distance\n",
        "*   Jaro Distance\n",
        "*   Jaro-Winkles Distance\n",
        "\n",
        "**FuzzyWuzzy**\n",
        "\n",
        "*   Partial Ratio\n",
        "*   Token Sort Ratio\n",
        "*   Token Set Ratio\n",
        "\n",
        "Para dar um conselho geral sobre essas métricas, recomendamos o uso da Jellyfish para comparar palavras individualmente (não frases completas), em aplicações como correção gramatical, por exemplo.\n",
        "\n",
        "Vamos tentar essas abordagens em alguns exemplos.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_R_rPhGqByN"
      },
      "source": [
        "# instalando a biblioteca jellyfish\n",
        "!pip -q install jellyfish\n",
        "\n",
        "import jellyfish\n",
        "\n",
        "# agora podemos usar suas funções para comparar algumas strings\n",
        "print(\"Levenshtein Distance\", jellyfish.levenshtein_distance(\"jellyfish\", \"smellyfish\"))\n",
        "print(\"Jaro Distance\", jellyfish.jaro_distance(\"jellyfish\", \"smellyfish\"))\n",
        "print(\"Jaro-Winkler Distance\", jellyfish.jaro_winkler(\"jellyfish\", \"smellyfish\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i-MVdydqVK4"
      },
      "source": [
        "É importante notar que a faixa de valores das saídas são diferentes e as próprias saídas significam coisas diferentes. Por exemplo, na Levenshtein Distance é medido quantas edições você precisa fazer em uma palavra para transformá-la na outra (no nosso caso, apague \"J\" e digite \"S\"). Jaro e Jaro-Winkler calculam o grau de similaridade com base em algumas estatísticas sobre as palavras. É importante entender esses detalhes para lidar melhor com o output.\n",
        "\n",
        "Para comparar duas sentenças, FuzzyWuzzy é mais adequado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HRLXFkZrYjz"
      },
      "source": [
        "# instalando a biblioteca fuzzywuzzy\n",
        "!pip -q install fuzzywuzzy\n",
        "\n",
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "sentence_a = \"Três amigos foram ao supermercado para comprar pizzas\"\n",
        "sentence_b = \"Duas pizzas foram compradas por três amigos no supermercado local\"\n",
        "\n",
        "# We can now compare full sentences with different approaches of the package\n",
        "print(\"Partial Ratio: \", fuzz.partial_ratio(sentence_a, sentence_b))\n",
        "print(\"Token Sort Ratio: \", fuzz.token_sort_ratio(sentence_a, sentence_b))\n",
        "print(\"Token Set Ratio: \", fuzz.token_set_ratio(sentence_a, sentence_b))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE2uYBqOrwx2"
      },
      "source": [
        "Observe como os resultados são diferentes. É devido à matemática por trás dos métodos fuzzy. Para obter similaridade semântica de duas sentenças, é recomendado usar a função Token Set Ratio. Como você pode ver, ela pode capturar muito bem duas strings semânticas equivalentes, escritas de maneira muito diferente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfHEOcKUvHCH"
      },
      "source": [
        "## 2.3 Etapas básicas de projetos de NLP\n",
        "\n",
        "Fazer qualquer coisa complicada em Machine Learning geralmente significa construir um pipeline. A ideia é dividir seu problema em partes muito pequenas e usar o Machine Learning para resolver cada uma delas separadamente. Então, encadeando vários modelos de aprendizado de máquina que se alimentam uns aos outros, você pode fazer coisas muito complicadas.\n",
        "\n",
        "Normalmente, em um projeto de NLP, há uma série de etapas iniciais comuns à maioria das aplicações. Em primeiro lugar, você precisará de um banco de dados para trabalhar. Em seguida, a próxima etapa é pré-processar seus dados. A etapa de pré-processamento também está aqui, mas exige subetapas exclusivas, como Stemming/Lemmatization (converter palavras em sua forma raiz), remoção de stopwords (eliminar palavras irrelevantes) e outras técnicas de feature engineering específicas de NLP. Com seu texto pré-processado, você terá que transformá-lo em features reais com técnicas como TF-IDF ou Word Embeddings.\n",
        "\n",
        "Depois de preparar os dados, finalmente, é possível usá-los para construir o modelo para a aplicação desejada (Análise de Sentimento, no nosso caso)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of8_SUSa22mY"
      },
      "source": [
        "## 2.4 Coletando e explorando os dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8ZP9uwCx41H"
      },
      "source": [
        "### 2.4.1 API do Twitter\n",
        "\n",
        "Para realizar nossa análise de sentimento, usaremos dados adquiridos do Twitter sobre as vacinas da Rússia e Oxford.\n",
        "\n",
        "Os dados foram obtidos pela API do twitter, com auxílio da biblioteca *tweepy*.\n",
        "\n",
        "É importante notar que existem várias outras maneiras de obter dados de texto. Por exemplo, OCR (Reconhecimento óptico de caracteres) é uma ferramenta muito útil que você pode aplicar a arquivos de documentos para extrair dados de texto. Ele também pode ser obtido por entrada do usuário, como comentários em seu site.\n",
        "\n",
        "Observação: Twitter já foi uma importante fonte de estudos para machine learning, pois se utilizava os tweets como referência de análise de comportamento e percepção das pessoas sobre diferentes eventos (desastres naturais, eleições, etc...). Em 2023 a plataforma limitou o acesso de desenvolvedores livres aos seus dados.\n",
        "\n",
        "[Conta de Desenvolvedor](https://developer.twitter.com/en)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EuWSYBlTSYb"
      },
      "source": [
        "###  2.4.2 Importando e analisando os dados\n",
        "\n",
        "Vamos usar um banco de dados do twitter pré-baixado com cerca de doze mil tweets, contido no arquivo *vacina_tweets_sentiment.xlsx*. Além dos recursos trazidos pelos tweets, também temos o rótulo de sentimento para cada tweet, obtido por meio do Amazon Comprehend da AWS. Esses rótulos serão nossa verdade fundamental para construir nosso próprio modelo de análise de sentimento.\n",
        "\n",
        "É importante ressaltar que estamos utilizando dados reais de tweets obtidos pela API e que o conteúdo de nenhum deles representa as opiniões e posições da VAI Academy ou dos seus membros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzWRs7zPTS5s"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel('dados/vacina_tweets_sentiment.xlsx', usecols=['text', 'len_text', 'sentiment', 'vacina', 'conf_positive', 'conf_negative', 'conf_neutral', 'conf_mixed'])\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_1Nq0SWNUDS"
      },
      "source": [
        "Vamos ver uma breve análise exploratória de nossas variáveis.\n",
        "\n",
        "Primeiro, vamos descobrir como é a distribuição de rótulos em nosso conjunto de dados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmOWRCOfYaj1"
      },
      "source": [
        "df['sentiment'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G23AwbZUxP62"
      },
      "source": [
        "Podemos ver que temos muito mais tweets **neutros** sobre as vacinas do que **positivos** e **negativos**. Vamos ver esses sentimentos distribuídos pelo tipo de vacina."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMjIoZKI4Ztp"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "df_count = df.groupby(['vacina', 'sentiment'], as_index=False)['text'].count()\n",
        "df_count.columns = ['vacina', 'sentiment', 'count']\n",
        "\n",
        "sns.set(style='whitegrid')\n",
        "ax = sns.barplot(data=df_count, x='sentiment', y='count', hue='vacina')\n",
        "ax.set(xlabel='Sentimento', ylabel='Contagem dos sentimentos')\n",
        "ax.legend(title='Vacina', loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrYCLaaK4aBS"
      },
      "source": [
        "Outra visualização possível é quanto ao número de caracteres. Existe alguma relação entre o tamanho do texto e o sentimento?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9xA86o2xO06"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "ax = sns.boxplot(data=df, x=\"len_text\", y=\"sentiment\")\n",
        "ax.set(xlabel='Tamanho do texto', ylabel='Sentimento')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHs5Fnwuz1Ro"
      },
      "source": [
        "Podemos ver uma leve diferença no tamanho dos textos de acordo com o sentimento, mas nada que chame muita atenção.\n",
        "\n",
        "Vamos verificar os 5 principais comentários de cada rótulo, para cada vacina. Aqui podemos ter uma visão mais clara sobre o que os usuários do Twitter estão pensando sobre as vacinas.\n",
        "\n",
        "Vamos começar com a vacina russa. Podemos filtrar os principais comentários positivos, avaliados pela quantidade de confiança que recebemos da AWS de que este é um comentário positivo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwoD_umSSAKc"
      },
      "source": [
        "df_russia = df[df['vacina']=='russia']\n",
        "\n",
        "df_russia = df_russia.sort_values(by=['conf_positive'], ascending=False)\n",
        "\n",
        "df_russia['text'].to_list()[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbPXOuHoT7VF"
      },
      "source": [
        "Vamos dar uma olhada nas outras classes de tweets sobre a vacina da Russia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0j8MeuiUKS1"
      },
      "source": [
        "df_russia = df_russia.sort_values(by=['conf_negative'], ascending=False)\n",
        "\n",
        "df_russia['text'].to_list()[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "206ng3iTUK8E"
      },
      "source": [
        "df_russia = df_russia.sort_values(by=['conf_neutral'], ascending=False)\n",
        "\n",
        "df_russia['text'].to_list()[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROYwWZLcUKqh"
      },
      "source": [
        "df_russia = df_russia.sort_values(by=['conf_mixed'], ascending=False)\n",
        "\n",
        "df_russia['text'].to_list()[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEKljwX0UdUK"
      },
      "source": [
        "É bom notar alguns detalhes sobre os comentários. De fato, o sentimento dos comentários negativos é negativo, mas não sobre a própria Rússia, e sim sobre outras coisas do cenário político e da saúde do Brasil.\n",
        "\n",
        "Além disso, os comentários mistos e neutros são de fato o que esperávamos. Os comentários neutros são tweets informativos. Os comentários mistos têm sentimentos bons e ruins no texto. Os principais comentários positivos têm outros tópicos além da própria vacina.\n",
        "\n",
        "Agora, vamos dar uma olhada na opinião sobre as vacinas de Oxford."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycY4kVVwU8xT"
      },
      "source": [
        "df_oxford = df[df['vacina']=='oxford']\n",
        "\n",
        "df_oxford = df_oxford.sort_values(by=['conf_positive'], ascending=False)\n",
        "\n",
        "df_oxford['text'].to_list()[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVyobO8pU_oD"
      },
      "source": [
        "df_oxford = df_oxford.sort_values(by=['conf_negative'], ascending=False)\n",
        "\n",
        "df_oxford['text'].to_list()[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7anYZ9vUVAC6"
      },
      "source": [
        "df_oxford = df_oxford.sort_values(by=['conf_neutral'], ascending=False)\n",
        "\n",
        "df_oxford['text'].to_list()[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2JlUzUuVAZu"
      },
      "source": [
        "df_oxford = df_oxford.sort_values(by=['conf_mixed'], ascending=False)\n",
        "\n",
        "df_oxford['text'].to_list()[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sv1KWPpoVov6"
      },
      "source": [
        "Novamente, recebemos tópicos mistos nos 5 principais tweets positivos, mas a opinião geral entre os 5 principais é de fato positiva para a vacina de Oxford.\n",
        "\n",
        "É bom notar como os comentários negativos são contra a vacina da Rússia, o que pode implicar que esses comentários negativos podem, afinal, ser um comentário positivo sobre a vacina de Oxford em alguns casos.\n",
        "\n",
        "Sobre os tweets neutros e mistos, temos o cenário semelhante: tweets informativos como neutros e textos com sentimentos mistos como mistos.\n",
        "\n",
        "Uma boa coisa a se fazer ao analisar dados textuais é gerar uma nuvem de palavras. Word Cloud é uma representação gráfica de suas palavras mais frequentes.\n",
        "\n",
        "Antes de gerá-la, vamos usar um pacote Python para transformar nosso texto em tokens. Tokens são pedaços menores de texto que ainda contêm informações. É muito comum usarmos cada palavra como tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4Nd0FKpg7CY"
      },
      "source": [
        "df['tokens'] = df['text'].apply(lambda x: x.split())\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZoJfizBiZVC"
      },
      "source": [
        "Com os tokens nós podemos gerar a Word Cloud para cada sentimento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vGHTiLUidd2"
      },
      "source": [
        "# vamos processar nossos dados para gerar a Word Cloud\n",
        "neutral_comments = ''\n",
        "negative_comments = ''\n",
        "positive_comments = ''\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    if row['sentiment'] == 'NEUTRAL':\n",
        "        neutral_comments += \" \".join(row['tokens']) + \" \"\n",
        "    elif row['sentiment'] == 'NEGATIVE':\n",
        "        negative_comments += \" \".join(row['tokens']) + \" \"\n",
        "    elif row['sentiment'] == 'POSITIVE':\n",
        "        positive_comments += \" \".join(row['tokens']) + \" \""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5K4CqSkionu"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "# definindo os visuais das nossas Word Clouds\n",
        "neutral_wordcloud = WordCloud(width=600, height=600, background_color='white',\n",
        "                              min_font_size=10,collocations=False).generate(neutral_comments)\n",
        "negative_wordcloud = WordCloud(width=600, height=600, background_color='white',\n",
        "                               min_font_size=10,collocations=False).generate(negative_comments)\n",
        "positive_wordcloud = WordCloud(width=600, height=600, background_color='white',\n",
        "                               min_font_size=10,collocations=False).generate(positive_comments)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KekLG1YhiwcP"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(30, 3*20))\n",
        "titles = ['NEUTRAL', 'NEGATIVE', 'POSITIVE']\n",
        "\n",
        "# agora podemos plotar as nossas Word Clouds\n",
        "for idx, wordcloud in enumerate([neutral_wordcloud, negative_wordcloud, positive_wordcloud]):\n",
        "    plt.subplot(1, 3, idx + 1)\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout(pad = 3)\n",
        "\n",
        "    # ax[idx].set_title(titles[idx])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exuedLbNat2b"
      },
      "source": [
        "## 2.5 Processamento de Dados\n",
        "\n",
        "Na última seção, foi possível dar uma olhada em nosso conjunto de dados. Como qualquer outra tarefa de Data Science, devemos pré-processar nossos dados para fazer o melhor uso deles. Mas primeiro, vamos pensar em uma questão: quais são as nossas features? Em tarefas de processamento de texto, nossas features são nossas palavras. Nosso conjunto de dados contém muitos textos e o que descreve suas informações são suas palavras. Queremos dizer ao nosso computador o que um texto significa quando tem um conjunto específico de palavras em uma ordem específica repetido um número específico de vezes. Isso pode dizer se um texto representa um comentário positivo em uma mídia social ou se ele está reclamando do preço do nosso produto, por exemplo.\n",
        "\n",
        "Um detalhe específico desse conjunto de dados é que ele pode conter URLs e tags do Twitter. Para análise de sentimento, URLs e tags não contêm muitas informações, portanto, é uma prática recomendada retirá-las do texto. Podemos fazer isso facilmente em Python com expressões regulares."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVimrg7U5c1J"
      },
      "source": [
        "# vamos usar o regex do Python para remover as URL's\n",
        "import re\n",
        "\n",
        "df = df.copy()\n",
        "df['text_preproc'] = df['text'].apply(lambda x: re.sub(r\"http\\S+\", \"\", str(x)))\n",
        "df['text_preproc'] = df['text_preproc'].apply(lambda x: re.sub('@[^\\s]+','',str(x)))\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVCiA0en5cab"
      },
      "source": [
        "Agora precisamos ter certeza de que estamos ensinando aos nossos computadores o que queremos que eles entendam sobre o nosso texto. Possivelmente queremos nosso modelo interpretando a palavra \"CASA\" (todos os caracteres em maiúsculo) da mesma forma que interpretamos a palavra \"casa\" (todos os caracteres em minúsculo). Portanto, uma primeira etapa trivial no pré-processamento pode ser definir todos os caracteres em nosso texto para letras minúsculas.\n",
        "\n",
        "Isso também se aplica à acentuação. Não queremos que nosso modelo subestime \"currículo\" e \"curriculo\" como features diferentes.\n",
        "\n",
        "Felizmente, o Python pode lidar com essas etapas de pré-processamento com muita facilidade.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unidecode é uma biblioteca do Python que usaremos para remover acentuações\n",
        "!pip install unidecode"
      ],
      "metadata": {
        "id": "bpvKn2OdLiFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfR5BW11d66h"
      },
      "source": [
        "from unidecode import unidecode\n",
        "\n",
        "# transformar caracteres em minúscolo\n",
        "df['text_preproc'] = df['text_preproc'].apply(lambda x: x.lower())\n",
        "# remover acentuação\n",
        "df['text_preproc'] = df['text_preproc'].apply(lambda x: unidecode(x))\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qgGGmZkd_Iu"
      },
      "source": [
        "Outra coisa que podemos querer eliminar são palavras e caracteres que não contêm tantas informações. Há uma grande chance de que palavras irrelevantes (\"o\", \"a\", \"um\", \"e\", \"este\", \"estes\", etc) não sejam úteis para descrever informações em nosso conjunto de dados. O mesmo se aplica à pontuação (\".\", \"!\", \"?\", \",\" etc). Portanto, é melhor eliminar essas palavras e caracteres do nosso conjunto de dados, mantendo apenas palavras que de fato contenham informações.\n",
        "\n",
        "Para remover as palavras irrelevantes, teremos que transformar cada texto em uma lista de palavras e, em seguida, obter uma lista de palavras irrelevantes de nosso idioma específico do NLTK e filtrar nosso texto original removendo essas palavras irrelevantes. Para trabalhar com NLTK, primeiro teremos que instalá-lo com pip e também usar sua API para baixar a lista de palavras irrelevantes. No universo de NLP, o conjunto de dados de texto usado para manipular nossos dados é normalmente chamado de \"corpora\" ou \"corpus\"."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install nltk"
      ],
      "metadata": {
        "id": "dyfWrOPILkcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_4RhNzwUJP-"
      },
      "source": [
        "# usando NLTK download() para baixar o nosso corpora\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed9zo-kXeqC8"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# essa função realiza todos os passos de remoção de stopwords\n",
        "def remove_stopwords(text):\n",
        "\n",
        "    # separa o texto em uma lista de palavras\n",
        "    words = text.split(' ')\n",
        "\n",
        "    # coletando a lista de stopwords para o português\n",
        "    stop_words = set(stopwords.words('portuguese'))\n",
        "\n",
        "    # filtrando nosso texto, removendo as stopwords\n",
        "    words_filtered = [w for w in words if w not in stop_words]\n",
        "\n",
        "    # juntando a lista de palavras em texto novamente\n",
        "    text_filtered = ' '.join(words_filtered)\n",
        "\n",
        "    return text_filtered\n",
        "\n",
        "# agora, podemos usar o apply() para remover stopwords do nosso texto\n",
        "df['text_preproc'] = df['text_preproc'].apply(lambda x: remove_stopwords(x))\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1OPlJ7mVAoO"
      },
      "source": [
        "Muito bom! Vamos agora remover as pontuações com a biblioteca String"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKptdtTEU8zr"
      },
      "source": [
        "# Importando a biblioteca string para remover pontuação\n",
        "import string\n",
        "\n",
        "# remover pontuação do nosso texto\n",
        "df['text_preproc'] = df['text_preproc'].apply(lambda x: x.translate(\n",
        "    str.maketrans('', '', string.punctuation)))\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0oWevCZfXmC"
      },
      "source": [
        "Normalmente, os idiomas trabalham com adição/inflexão nas palavras raiz. Tomando como exemplo a palavra \"correr\", podemos encontrar inflexões como \"corrida\", \"correndo\" ou \"corri\" em uma frase. Podemos querer que nosso modelo entenda essas palavras como o mesmo recurso. Para isso, podemos aplicar o Stemming em nosso texto. Isso reduzirá cada palavra à sua raiz, sem afixos como -a, -as, -o, -os, -i, -er. Observe que nem toda raiz é uma palavra compreensível para humanos, mas pode perfeitamente conter informações de sua frase.\n",
        "\n",
        "Você pode inferir que a derivação é um pré-processamento muito específico da linguagem. Felizmente, o pacote NLTK cobre muitos idiomas como inglês, português ou francês. Existem também algumas opções de Stemmers que você pode escolher. Para o idioma inglês, você pode usar Porter ou Lancaster. A principal diferença entre os dois é que Lancaster Stemmer é muito mais agressivo ao transformar palavras em sua raiz. Em algumas aplicações, ele pode gerar recursos mais úteis, mas em outros, irá descaracterizar seu texto. Para o idioma português, você pode usar o RSLP Portuguese Stemmer. Em aplicativos do mundo real, você pode avaliar quais Stemmers funcionam melhor com seus dados e modelos.\n",
        "\n",
        "Em Python, o pacote NLTK disponibiliza o uso dessas ferramentas de Stemming de uma maneira muito fácil.\n",
        "\n",
        "Vamos definir novamente uma função para aplicar o stemmer ao nosso texto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfkqQ0Haeq4R"
      },
      "source": [
        "import nltk\n",
        "nltk.download('rslp')\n",
        "\n",
        "#  definindo uma função para aplicar o stemming ao nosso texto\n",
        "def do_stem(text):\n",
        "\n",
        "    # separando o texto em uma lista de palavras\n",
        "    words = text.split(' ')\n",
        "\n",
        "    stemmer = nltk.stem.RSLPStemmer()\n",
        "\n",
        "    # aplicando o stemming no texto\n",
        "    words_stem = [stemmer.stem(w) for w in words if len(w)>0]\n",
        "\n",
        "    # juntando a lista de palavras em texto novamente\n",
        "    text_stem = ' '.join(words_stem)\n",
        "\n",
        "    return text_stem\n",
        "\n",
        "df['text_preproc'] = df['text_preproc'].apply(lambda text: do_stem(text))\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR8zCxKUWiov"
      },
      "source": [
        "## 2.6 Representações matemática e informações do texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1XemhFCpEvo"
      },
      "source": [
        "### 2.6.1 Tokenization\n",
        "\n",
        "Depois que os dados forem normalizados, precisamos saber como representá-los como features. O processo de tokenização consiste em dividir strings mais longas em pequenos pedaços significativos chamados tokens. A forma mais comum de tokenizar um texto é dividi-lo em palavras, ou seja, dado um pedaço de texto, o processo de tokenização retornará uma lista de palavras. Vamos ver como tokenizar nossos dados normalizados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR4AOxEgWzLf"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# aplicando o tokenizer\n",
        "df['tokens'] = df['text_preproc'].apply(word_tokenize)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_5Aw6fNvyaD"
      },
      "source": [
        "### 2.6.2 Separando nossos dados\n",
        "\n",
        "Antes de continuarmos, vamos separar nossos dados entre dados de treino e teste."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_w-MmYvwKvu"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df[['text_preproc', 'tokens']]\n",
        "\n",
        "y = df[['sentiment']]\n",
        "\n",
        "TEST_SIZE = 0.3\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT5ueaTZtcu_"
      },
      "source": [
        "### 2.6.3 Bag of Words\n",
        "\n",
        "Algoritmos de Machine Learning tomam features numéricas como entrada, portanto, será necessário representar o texto em uma forma numérica. Com o modelo Bag of Words (BoW), podemos pegar a saída de tokenização e transformar as listas em um espaço vetorial de todos os tokens exclusivos. Esse espaço vetorial é chamado de vocabulário. Portanto, para uma determinada frase, calculamos quantas vezes cada palavra aparece nos índices da lista, onde cada entrada corresponde a uma palavra no vocabulário. Vamos ver na prática como funciona."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iq8dk1IQtdIP"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "BoW = CountVectorizer()\n",
        "\n",
        "# treinando o modelo de vetorização e transformando os dados de treino\n",
        "BoW_X_train = BoW.fit_transform(X_train['text_preproc']).toarray()\n",
        "\n",
        "# aplicando o modelo treinado aos dados de teste\n",
        "BoW_X_test  = BoW.transform(X_test['text_preproc']).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R93g6aFZwhKj"
      },
      "source": [
        "BoW_X_train = pd.DataFrame(data=BoW_X_train, columns=BoW.get_feature_names_out())\n",
        "\n",
        "BoW_X_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61hJ9pvMAHu_"
      },
      "source": [
        "### 2.6.4 Term Frequency - Inverse Document Frequency (TF-IDF)\n",
        "\n",
        "Term Frequency Inverse Document Frequency (TF-IDF) é aplicada a um BoW para determinar a frequência relativa para palavras em um documento específico quando comparada à proporção inversa dessa palavra sobre todos os documentos na coleção. Assim, pode-se determinar a importância das palavras em um determinado documento, conforme mostrado a seguir.\n",
        "\n",
        "$ w_{i, j} = \\text{tf}_{i, j} \\times \\log\\left(\\dfrac{N}{\\text{df}_{i}}\\right) \\text{.} $\n",
        "\n",
        "Onde, a frequência do termo, $\\text{tf}_{i, j}$, é quantas vezes a palavra de índice $i$ no documento de índice $j$. A frequência do documento, $\\text{df}_{i}$, é o número de vezes que a palavra de índice ${i}$ está presente nos vocabulário de cada documento. E, finalmente, $N$ é a quantidade de documentos na sua coleção, com mais documentos, esse valor pode crescer absurdamente, então aplica-se a função log para reduzir esse efeito.\n",
        "\n",
        "Agora, vamos ver como aplicar o TF-IDF no Python!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfYqX-TWAIEK"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "Tfidf = TfidfVectorizer()\n",
        "\n",
        "# treinando o modelo de vetorização e transformando os dados de treino\n",
        "Tfidf_X_train = Tfidf.fit_transform(X_train['text_preproc']).toarray()\n",
        "\n",
        "# aplicando o modelo treinado aos dados teste\n",
        "Tfidf_X_test  = Tfidf.transform(X_test['text_preproc']).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "939cNP8_wmLA"
      },
      "source": [
        "Tfidf_X_train = pd.DataFrame(data=Tfidf_X_train, columns=Tfidf.get_feature_names_out())\n",
        "\n",
        "Tfidf_X_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8bOs-xDC641"
      },
      "source": [
        "### 2.6.5 Word Embedding\n",
        "\n",
        "Os métodos de vetorização como BoW e TF-IDF podem ser muito úteis, mas não conseguem representar o contexto das palavras. Isso significa que as mesmas palavras usadas em contextos diferentes têm a mesma representação, da mesma forma que palavras diferentes usadas com o mesmo significado são representadas de forma diferente. Além disso, um método de codificação one-hot, como BoW, apresenta uma representação muito esparsa com alta dimensionalidade.\n",
        "\n",
        "Word Embedding é uma técnica para representar palavras em vetores capazes de capturar o contexto das palavras em um documento. Também é capaz de suavizar o efeito de alta dimensionalidade usando um vetor muito mais compacto para representar as palavras.\n",
        "\n",
        "Existem três maneiras mais conhecidas de realizar um bom Word Embedding, mas abordaremos apenas uma delas. Word2vec é um modelo que foi pré-treinado em um corpus muito grande e fornece embeddings que mapeiam palavras semelhantes próximas umas das outras.\n",
        "\n",
        "Para isso, vamos utilizar o arquivo *skip_s50.txt* disponibilizado junto ao notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swDC4hicC8D1"
      },
      "source": [
        "!pip -q install gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYTao_H5AQpx"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# carregando o modelo de referência\n",
        "word2vec = KeyedVectors.load_word2vec_format(\"dados/skip_s50.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJz8rMtTAQiE"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# essas funções nos ajudam a coletar os word embeddings\n",
        "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=50):\n",
        "    if len(tokens_list)<1:\n",
        "        return np.zeros(k)\n",
        "    if generate_missing:\n",
        "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
        "    else:\n",
        "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
        "    length = len(vectorized)\n",
        "    summed = np.sum(vectorized, axis=0)\n",
        "    averaged = np.divide(summed, length)\n",
        "    return averaged\n",
        "\n",
        "def get_word2vec_embeddings(vectors, clean_questions, generate_missing=False):\n",
        "    embeddings = clean_questions['tokens'].apply(lambda x: get_average_word2vec(x, vectors,\n",
        "                                                                                generate_missing=generate_missing))\n",
        "    return list(embeddings)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m52iDQePAXDl"
      },
      "source": [
        "# com as funções definidas acima, vamos obter nossos word embeddings\n",
        "word2vec_X_train = get_word2vec_embeddings(word2vec, X_train)\n",
        "word2vec_X_test = get_word2vec_embeddings(word2vec, X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rrqLCoRZdP_"
      },
      "source": [
        "<a id=\"analise_sent\"></a>\n",
        "# 3 Análise de Sentimentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLPzFj5Nol-F"
      },
      "source": [
        "Legal! Agora que construímos nossos dados, é hora de experimentar alguns modelos e verificar qual deles nos dá os melhores resultados. Nesta seção, exploraremos dois modelos que funcionam muito bem para esse tipo de tarefa: Naive Bayes e SVC.\n",
        "\n",
        "Observe como é fácil construir e avaliar esses modelos com o SKLearn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJHFYXBmGjI4"
      },
      "source": [
        "## 3.1 NaiveBayes e BoW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERpAnQonDFO1"
      },
      "source": [
        "# importando as bibliotecas\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "\n",
        "# criando e treinando o modelo\n",
        "naive_bayes_classifier = MultinomialNB()\n",
        "naive_bayes_classifier.fit(BoW_X_train, y_train)\n",
        "\n",
        "# realizando a predição nos dados de teste\n",
        "y_pred = naive_bayes_classifier.predict(BoW_X_test)\n",
        "\n",
        "# finalmente, vamos ver os resultados do modelo\n",
        "score1 = metrics.accuracy_score(y_test, y_pred)\n",
        "print(score1)\n",
        "print(metrics.confusion_matrix(y_test, y_pred))\n",
        "print(metrics.classification_report(y_test, y_pred, zero_division=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTSvJ0HxpbqT"
      },
      "source": [
        "Para resultados acima, podemos ver que, embora o score de precisão tenha sido alto, o modelo não teve um bom desempenho na classificação dos comentários NEGATIVOS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89ZQhTlrLCBD"
      },
      "source": [
        "## 3.2 NaiveBayes e TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN2r3gg9LFoN"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "\n",
        "naive_bayes_classifier = MultinomialNB()\n",
        "naive_bayes_classifier.fit(Tfidf_X_train, y_train)\n",
        "y_pred = naive_bayes_classifier.predict(Tfidf_X_test)\n",
        "\n",
        "score1 = metrics.accuracy_score(y_test, y_pred)\n",
        "print(score1)\n",
        "print(metrics.confusion_matrix(y_test, y_pred))\n",
        "print(metrics.classification_report(y_test, y_pred, zero_division=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3qRpxYapqOM"
      },
      "source": [
        "O uso do TF-IDF ajudou o modelo na classificação dos comentários, mas ainda podemos tentar melhores resultados na classificação dos comentários NEGATIVOS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxFY2sWHLOSj"
      },
      "source": [
        "## 3.3 SVC e BoW\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4D_mqmz-LZN-"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn import metrics\n",
        "\n",
        "svc_classifier = SVC()\n",
        "svc_classifier.fit(BoW_X_train, y_train)\n",
        "y_pred = svc_classifier.predict(BoW_X_test)\n",
        "\n",
        "score1 = metrics.accuracy_score(y_test, y_pred)\n",
        "print(score1)\n",
        "print(metrics.confusion_matrix(y_test, y_pred))\n",
        "print(metrics.classification_report(y_test, y_pred, zero_division=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHGIVMnVp2Ft"
      },
      "source": [
        "O SVC funcionou bem como nosso modelo. Agora temos maiores chances de obter a classificação de sentimento certa para cada uma das classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkDwzy4iMFDc"
      },
      "source": [
        "## 3.4 SVC e TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHsrrqglMEZB"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn import metrics\n",
        "\n",
        "svc_classifier = SVC()\n",
        "svc_classifier.fit(Tfidf_X_train, y_train)\n",
        "y_pred = svc_classifier.predict(Tfidf_X_test)\n",
        "\n",
        "score1 = metrics.accuracy_score(y_test, y_pred)\n",
        "print(score1)\n",
        "print(metrics.confusion_matrix(y_test, y_pred))\n",
        "print(metrics.classification_report(y_test, y_pred, zero_division=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGS98v2vqB-v"
      },
      "source": [
        "Usando o TF-IDF nos dá um resultado levemente melhor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpDedB9aBe3U"
      },
      "source": [
        "## 3.5 SVC e Word2Vec\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn61PcFnBdTi"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn import metrics\n",
        "\n",
        "svc_classifier = SVC()\n",
        "svc_classifier.fit(word2vec_X_train, y_train)\n",
        "y_pred = svc_classifier.predict(word2vec_X_test)\n",
        "\n",
        "score1 = metrics.accuracy_score(y_test, y_pred)\n",
        "print(score1)\n",
        "print(metrics.confusion_matrix(y_test, y_pred))\n",
        "print(metrics.classification_report(y_test, y_pred, zero_division=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XS0SGAYPpQEo"
      },
      "source": [
        "Bem, o Word2Vec não ajudou muito a obter melhores resultados. Ficamos com SVC e TF-IDF.\n",
        "\n",
        "Parabéns, você construiu seu modelo de Análise de Sentimento!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rkz9kdtcoie"
      },
      "source": [
        "## 3.6 Explorando o modelo\n",
        "\n",
        "Legal! Obtivemos um bom resultado com SVC e TF-IDF. Agora, vamos usar nosso modelo para prever nosso conjunto de dados. Isso nos permitirá ter todo o nosso conjunto de dados rotulado exatamente como o resultado que obtivemos da AWS. Então, seremos capazes de analisar como nosso modelo funcionou fazendo nossa Análise de Sentimento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykNmzItudPpX"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn import metrics\n",
        "\n",
        "# vamos treinar o nosso classificador\n",
        "svc_classifier = SVC(probability=True)\n",
        "svc_classifier.fit(Tfidf_X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lD2OwH70hc08"
      },
      "source": [
        "# aplicando a transformação ao nosso dataset inteiro\n",
        "Tfidf_X = Tfidf.transform(df['text_preproc']).toarray()\n",
        "\n",
        "# classificando o sentimento\n",
        "y_pred = svc_classifier.predict(Tfidf_X)\n",
        "class_probabilities = svc_classifier.predict_proba(Tfidf_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nrwx67QVntez"
      },
      "source": [
        "# com classes_, podemos verificar a ordem que as probabilidades de cada classe estão definidas em class_probabilities\n",
        "svc_classifier.classes_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN5xfVuVeG59"
      },
      "source": [
        "# vamos criar o nosso dataframe com os resultados. Nele, termos o texto, a classificação do sentimento e as probabilidades para cada sentimento\n",
        "df_result = pd.DataFrame(class_probabilities, columns = ['conf_mixed', 'conf_negative', 'conf_neutral', 'conf_positive'])\n",
        "df_result['sentiment'] = y_pred\n",
        "df_result['text'] = df['text']\n",
        "df_result['vacina'] = df['vacina']\n",
        "df.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RjmfXntp_0T"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMk6d2dNpXww"
      },
      "source": [
        "Finalmente, vamos repetir a análise que fizemos antes, obtendo os 5 principais tweets para cada sentimento, desta vez com base em nosso próprio classificador."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCYwHYm-pp8K"
      },
      "source": [
        "df_russia = df_result[df_result['vacina']=='russia']\n",
        "\n",
        "df_russia = df_russia.drop_duplicates(subset=['text'])\n",
        "\n",
        "df_russia = df_russia.sort_values(by=['conf_positive'], ascending=False)\n",
        "\n",
        "df_russia['text'].to_list()[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp2Ony7xmtK7"
      },
      "source": [
        "df_russia = df_russia.sort_values(by=['conf_negative'], ascending=False)\n",
        "\n",
        "df_russia['text'].to_list()[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esenCBTDmtq9"
      },
      "source": [
        "df_russia = df_russia.sort_values(by=['conf_neutral'], ascending=False)\n",
        "\n",
        "df_russia['text'].to_list()[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ph_KhW7LmuEL"
      },
      "source": [
        "df_russia = df_russia.sort_values(by=['conf_mixed'], ascending=False)\n",
        "\n",
        "df_russia['text'].to_list()[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2vFFJJ0m0b2"
      },
      "source": [
        "df_oxford = df_result[df_result['vacina']=='oxford']\n",
        "\n",
        "df_oxford = df_oxford.drop_duplicates(subset=['text'])\n",
        "\n",
        "df_oxford = df_oxford.sort_values(by=['conf_positive'], ascending=False)\n",
        "\n",
        "df_oxford['text'].to_list()[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91VkU987m5QU"
      },
      "source": [
        "df_oxford = df_oxford.sort_values(by=['conf_negative'], ascending=False)\n",
        "\n",
        "df_oxford['text'].to_list()[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldhAY7IYm5oM"
      },
      "source": [
        "df_oxford = df_oxford.sort_values(by=['conf_neutral'], ascending=False)\n",
        "\n",
        "df_oxford['text'].to_list()[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZzZtEfam6Aj"
      },
      "source": [
        "df_oxford = df_oxford.sort_values(by=['conf_mixed'], ascending=False)\n",
        "\n",
        "df_oxford['text'].to_list()[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzxurQFcDGRX"
      },
      "source": [
        "<a id=\"sota_nlp\"></a>\n",
        "# 4 NLP Estado da Arte (SOTA - State-of-the-Art)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpZkWS3uGP9f"
      },
      "source": [
        "Neste notebook, fornecemos uma visão geral das técnicas básicas que você deve saber para construir suas soluções de NLP e o ajudamos a construir sua aplicação de Análise de Sentimentos. Aqui, veremos como o NLP evoluiu com técnicas avançadas de IA, permitindo-nos usar algoritmos poderosos para construir soluções de problemas.\n",
        "\n",
        "O projeto [Sentiment analysis neural network trained by fine-tuning BERT, ALBERT, or DistilBERT on the Stanford Sentiment Treebank](https://github.com/barissayil/SentimentAnalysis) é um excelente exemplo do que é possível contruir com essas tecnologias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPsHpXasFipa"
      },
      "source": [
        "## 4.1 Stanford NLP Group\n",
        "\n",
        "[Stanford NLP Group](https://nlp.stanford.edu/) é um grupo de pessoas que trabalham para criar soluções de NLP robustas. Eles contribuem com um grande conjunto de bases de dados e soluções construídas sobre técnicas avançadas de aprendizado de máquina. Na [página de Análise de Sentimentos](https://nlp.stanford.edu/sentiment/) deles, é possível testar o modelo de [RNN em Análise de Sentimentos](https://nlp.stanford.edu/sentiment/treebank.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLcKROwJFmHj"
      },
      "source": [
        "## 4.2  BERT\n",
        "\n",
        "Google é uma das empresas líderes nas pesquisas de NLP. Seu modelo de representação de linguagem [BERT](https://arxiv.org/pdf/1810.04805.pdf) acabou sendo muito útil na representação da linguagem humana e encorajou o desenvolvimento de seus modelos derivados [ALBERT](https://arxiv.org/pdf/1909.11942.pdf) e [DistilBERT](https://arxiv.org/pdf/1910.01108.pdf).\n",
        "."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG1KGJpNFqub"
      },
      "source": [
        "## 4.3 Hugging Face\n",
        "[Hugging Face](https://huggingface.co/) é um grupo obrigatório para todos os desenvolvedores de NLP. Eles estão construindo soluções incríveis usando técnicas avançadas de aprendizado de máquina, como o Zero-Shot Pipeline, que você pode até experimentar em seus [Collab Notebook](https://colab.research.google.com/drive/1jocViLorbwWIkTXKwxCOV9HLTaDDgCaw?usp=sharing).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8nWhNpwFwvJ"
      },
      "source": [
        "## 4.4 GPT-3\n",
        "\n",
        "[GPT-3](https://github.com/openai/gpt-3) é um modelo de Machine Learning criado pela [OpenAI](https://openai.com/) que pode escrever desde poemas até código. Agora está em beta privado e para você experimentar a API você precisa se cadastrar no site e aguardar a aceitação, mas você pode dar uma olhada no poder do GPT-3 nas redes sociais.\n",
        "\n",
        "GPT-3 trouxe com seus resultados impressionantes um conjunto de questões morais. MIT Technology Review escreveu um [overview](https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/) bem interessante sobre a tecnologia, abordando seu poder e também seus perigos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS2g2xcbsCBX"
      },
      "source": [
        "<a id=\"digdeeper\"></a>\n",
        "# 5 Dicas e Referências"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "550HF4SkqzFS"
      },
      "source": [
        "## 5.1 Dicas\n",
        "\n",
        "Nosso notebook cobriu muitos aspectos de NLP, mas ainda existe um universo inteiro que não abordamos aqui. Além disso, há algumas coisas que sugerimos que você tenha em mente ao desenvolver seus aplicativos. A lista a seguir é uma compilação de dicas e truques que você pode usar ao trabalhar com NLP:\n",
        "\n",
        "\n",
        "* Fique de olho nas novas técnicas e tecnologias da comunidade. Quem sabe um deles pode realmente resolver seus problemas.\n",
        "* Fique de olho nos impactos dessas novas tecnologias na sociedade. Há muitas questões morais envolvidas e é muito importante que tomemos cuidado com isso.\n",
        "* Conheça as soluções básicas antes de pular para as mais avançadas. Como vimos, soluções simples (modelos e pré-processamento, por exemplo) podem realmente ter um bom desempenho para texto.\n",
        "* Não use as etapas mostradas aqui como uma checklist. Use-as como referência. Você precisa ter um conhecimento profundo sobre onde deseja chegar com seus modelos e como trabalhar com seu texto para alcançá-lo.\n",
        "* Seja criativo. Para isso, estude outras soluções. Existem várias maneiras de coletar dados textuais e trabalhar com eles na comunidade open-source. Use-a para obter melhores resultados.\n",
        "* Seja muito cético. Existem muitos tutoriais e exemplos na internet que mostram resultados maravilhosos mas, se você olhar mais de perto, há erros cometidos ao longo da construção da aplicação que deram essa ilusão de um bom resultado. Novamente, tenha um conhecimento de base sólido e julgue cada etapa e cada exemplo que você vê. Esta é uma ótima maneira de contribuir com a comunidade e aprender."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuO8XReim1Ta"
      },
      "source": [
        "\n",
        "## 5.2 Referências\n",
        "\n",
        "**Links gerais:**\n",
        "\n",
        "Stanford NLP Group: https://nlp.stanford.edu/\n",
        "\n",
        "Hugging Face: https://huggingface.co/\n",
        "\n",
        "Google BERT Paper: https://arxiv.org/pdf/1810.04805.pdf\n",
        "\n",
        "OpenAI Machine Learning Language Generator GPT 3: https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/\n",
        "\n",
        "NLP On The Office Series: https://towardsdatascience.com/nlp-on-the-office-series-cf0ed44430d1\n",
        "\n",
        "**Repositórios GitHub:**\n",
        "\n",
        "barissayil/SentimentAnalysis: https://github.com/barissayil/SentimentAnalysis\n",
        "\n",
        "balavenkatesh3322/NLP-pretrained-model: https://github.com/balavenkatesh3322/NLP-pretrained-model\n",
        "\n",
        "**Livros e Cursos:**\n",
        "\n",
        "Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit (English Edition)\n",
        "\n",
        "Udacity Introduction to Machine Learning: https://www.udacity.com/course/intro-to-machine-learning--ud120\n"
      ]
    }
  ]
}